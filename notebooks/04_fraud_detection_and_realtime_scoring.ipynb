{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4767b95",
   "metadata": {},
   "source": [
    "# FinRisk: Credit Risk & Fraud Detection - Notebook 6\n",
    "## Phase 3: Fraud Detection & Real-time Scoring\n",
    "\n",
    "**Objective:**\n",
    "1.  Load the pre-trained champion credit risk model (Logistic Regression).\n",
    "2.  Build an unsupervised anomaly detection model for fraud using `IsolationForest`.\n",
    "3.  Simulate a real-time API endpoint that receives a transaction and returns a combined risk decision (credit risk + fraud risk).\n",
    "4.  Outline an A/B testing framework and a simple alerting mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "816b5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 1. Import Libraries\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib # For saving and loading models\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c293c",
   "metadata": {},
   "source": [
    "## 2. Load Data and Pre-trained Credit Model\n",
    "\n",
    "First, we load the necessary datasets. Then, to simulate a production environment, we will \"load\" our best-performing credit risk model from Phase 2. For this notebook, we'll quickly retrain the `Logistic Regression (Balanced)` model and save it to a file to simulate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d712f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data loaded successfully.\n",
      "\n",
      "--- Training and saving the champion credit risk model ---\n",
      "Credit risk model trained and saved to ../models/champion_credit_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- Load Datasets ---\n",
    "PROCESSED_DATA_PATH = '../data/processed/'\n",
    "MODELS_PATH = '../models/' # Let's create a new directory for models\n",
    "\n",
    "if not os.path.exists(MODELS_PATH):\n",
    "    os.makedirs(MODELS_PATH)\n",
    "\n",
    "try:\n",
    "    credit_features_df = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, 'credit_risk_features.csv'), parse_dates=['application_date'])\n",
    "    fraud_features_df = pd.read_csv(os.path.join(PROCESSED_DATA_PATH, 'fraud_detection_features.csv'))\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Please run previous notebooks first.\")\n",
    "    assert False, \"Data not found.\"\n",
    "\n",
    "# --- Simulate Loading a Pre-trained Credit Model ---\n",
    "print(\"\\n--- Training and saving the champion credit risk model ---\")\n",
    "\n",
    "# Define features (X) and target (y) from the credit dataset\n",
    "TARGET = 'default_flag'\n",
    "features_to_exclude = [\n",
    "    'application_id', 'customer_id', 'application_date', 'last_activity_date',\n",
    "    'default_flag', 'application_status', 'city'\n",
    "]\n",
    "X = credit_features_df.drop(columns=features_to_exclude)\n",
    "y = credit_features_df[TARGET]\n",
    "\n",
    "# Identify feature types\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create and train the pipeline\n",
    "credit_risk_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear'))\n",
    "])\n",
    "credit_risk_model.fit(X, y) # Train on the full dataset for \"production\"\n",
    "\n",
    "# Save the model\n",
    "model_filename = os.path.join(MODELS_PATH, 'champion_credit_model.joblib')\n",
    "joblib.dump(credit_risk_model, model_filename)\n",
    "\n",
    "print(f\"Credit risk model trained and saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61780fa5",
   "metadata": {},
   "source": [
    "## 3. Build Fraud Detection Model (Anomaly Detection)\n",
    "\n",
    "Fraud is a rare event, making it a perfect use case for anomaly detection. We will use `IsolationForest`, which is efficient at identifying unusual data points. We train this on the customer-level aggregated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540aa3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Building the fraud detection model ---\n",
      "Fraud detection model trained and saved to ../models/fraud_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Building the fraud detection model ---\")\n",
    "\n",
    "# Features for the fraud model are the aggregated transaction stats\n",
    "X_fraud = fraud_features_df.drop(columns=['customer_id'])\n",
    "\n",
    "# Isolation Forest works by \"isolating\" outliers.\n",
    "# The `contamination` parameter is an estimate of the percentage of anomalies in the data.\n",
    "# Based on the project brief, fraud rate is ~0.1%.\n",
    "fraud_detection_model = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.001, # Set to expected fraud rate\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "fraud_detection_model.fit(X_fraud)\n",
    "\n",
    "# Save the fraud model\n",
    "fraud_model_filename = os.path.join(MODELS_PATH, 'fraud_detection_model.joblib')\n",
    "joblib.dump(fraud_detection_model, fraud_model_filename)\n",
    "print(f\"Fraud detection model trained and saved to {fraud_model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382651f",
   "metadata": {},
   "source": [
    "## 4. Simulate Real-Time Scoring API\n",
    "\n",
    "This is the core of our production system. We'll create a function that simulates an API call. It will take new application/transaction data, use our models to score it, and return a decision based on business rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f216ae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "\n",
      "--- Scoring request for Customer: CUST_007999 ---\n",
      "{'customer_id': 'CUST_007999', 'decision': 'Error', 'reason': 'Customer not found'}\n",
      "\n",
      "--- Scoring request for Customer: CUST_008172 ---\n",
      "{'customer_id': 'CUST_008172', 'decision': 'Error', 'reason': 'Customer not found'}\n"
     ]
    }
   ],
   "source": [
    "# Load the models back as if we were in a separate API application\n",
    "loaded_credit_model = joblib.load(os.path.join(MODELS_PATH, 'champion_credit_model.joblib'))\n",
    "loaded_fraud_model = joblib.load(os.path.join(MODELS_PATH, 'fraud_detection_model.joblib'))\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "# Prepare data for quick lookups (simulating a database/cache)\n",
    "credit_data_lookup = credit_features_df.set_index('customer_id')\n",
    "fraud_data_lookup = fraud_features_df.set_index('customer_id')\n",
    "\n",
    "\n",
    "def real_time_scoring_api(customer_id: str):\n",
    "    \"\"\"\n",
    "    Simulates a real-time scoring API endpoint.\n",
    "    - Fetches customer data.\n",
    "    - Gets a credit risk score.\n",
    "    - Gets a fraud score.\n",
    "    - Applies business rules to make a decision.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Scoring request for Customer: {customer_id} ---\")\n",
    "    try:\n",
    "        # 1. Fetch data for the customer\n",
    "        customer_credit_features = credit_data_lookup.loc[[customer_id]].drop(columns=features_to_exclude)\n",
    "        customer_fraud_features = fraud_data_lookup.loc[[customer_id]].drop(columns=['customer_id'])\n",
    "    except KeyError:\n",
    "        return {\"customer_id\": customer_id, \"decision\": \"Error\", \"reason\": \"Customer not found\"}\n",
    "\n",
    "    # 2. Score for Credit Risk (probability of default)\n",
    "    credit_prob_default = loaded_credit_model.predict_proba(customer_credit_features)[:, 1][0]\n",
    "    \n",
    "    # 3. Score for Fraud Risk (anomaly score)\n",
    "    # The score is -1 for anomalies (fraud) and 1 for inliers (not fraud).\n",
    "    fraud_score = loaded_fraud_model.predict(customer_fraud_features)[0]\n",
    "    \n",
    "    # 4. Apply Business Rules\n",
    "    decision = \"Approved\"\n",
    "    reason = \"Customer meets credit and fraud criteria.\"\n",
    "    \n",
    "    if fraud_score == -1:\n",
    "        decision = \"Manual Review\"\n",
    "        reason = \"High fraud risk detected. Transaction flagged for investigation.\"\n",
    "    elif credit_prob_default > 0.6: # Example threshold\n",
    "        decision = \"Declined\"\n",
    "        reason = f\"Credit risk too high. Probability of default: {credit_prob_default:.2%}\"\n",
    "        \n",
    "    return {\n",
    "        \"customer_id\": customer_id,\n",
    "        \"credit_default_probability\": f\"{credit_prob_default:.2%}\",\n",
    "        \"is_potential_fraud\": bool(fraud_score == -1),\n",
    "        \"decision\": decision,\n",
    "        \"reason\": reason\n",
    "    }\n",
    "\n",
    "# --- Example API Calls ---\n",
    "# Example of a good customer\n",
    "good_customer_id = 'CUST_007999' \n",
    "print(real_time_scoring_api(good_customer_id))\n",
    "\n",
    "# Example of another customer\n",
    "other_customer_id = 'CUST_008172'\n",
    "print(real_time_scoring_api(other_customer_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af242dbd",
   "metadata": {},
   "source": [
    "## 5. A/B Testing Framework and Alerting\n",
    "\n",
    "In a production environment, you never replace a model blindly. You test it.\n",
    "\n",
    "**A/B Testing Framework:**\n",
    "* **Champion vs. Challenger:** Our current `Logistic Regression` is the \"Champion\". A new model (e.g., an optimized XGBoost) would be the \"Challenger\".\n",
    "* **Traffic Splitting:** We would configure our API to route a small percentage of traffic (e.g., 5%) to the Challenger model. The remaining 95% still goes to the Champion.\n",
    "* **Performance Monitoring:** We would then monitor the key metrics (default rates, fraud capture rates) for both groups of customers over a period of time.\n",
    "* **Promotion:** If the Challenger significantly and consistently outperforms the Champion, it gets promoted to become the new Champion.\n",
    "\n",
    "**Alerting System:**\n",
    "The `real_time_scoring_api` already includes a decision for \"Manual Review\". In a real system, this would trigger an alert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0fa943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scoring request for Customer: CUST_004019 ---\n",
      "{'customer_id': 'CUST_004019', 'decision': 'Error', 'reason': 'Customer not found'}\n"
     ]
    }
   ],
   "source": [
    "def generate_alert(api_response: dict):\n",
    "    \"\"\"Generates an alert if the decision requires manual review.\"\"\"\n",
    "    if api_response.get(\"decision\") == \"Manual Review\":\n",
    "        customer_id = api_response.get(\"customer_id\")\n",
    "        reason = api_response.get(\"reason\")\n",
    "        print(\"\\n\" + \"=\"*20 + \" ALERT \" + \"=\"*20)\n",
    "        print(f\"ALERT: Manual investigation required for Customer {customer_id}.\")\n",
    "        print(f\"Reason: {reason}\")\n",
    "        print(\"=\"*47)\n",
    "\n",
    "# Example of an alert being triggered\n",
    "fraudulent_customer_id = 'CUST_004019' # This is a made up example for simulation\n",
    "# Let's create a fake high-risk profile for this customer to trigger the alert\n",
    "high_risk_profile = fraud_data_lookup.mean().to_frame().T\n",
    "high_risk_profile.index = [fraudulent_customer_id]\n",
    "fraud_data_lookup = pd.concat([fraud_data_lookup, high_risk_profile])\n",
    "\n",
    "credit_data_lookup = credit_features_df.set_index('customer_id') # reset index\n",
    "\n",
    "# Now, we manually \"flag\" this customer in the model's eyes\n",
    "# In a real scenario, their transaction data would naturally lead to this\n",
    "# Forcing the model to predict -1 for this made-up customer\n",
    "loaded_fraud_model.fit(high_risk_profile) \n",
    "\n",
    "# Run the API and generate the alert\n",
    "response = real_time_scoring_api(fraudulent_customer_id)\n",
    "print(response)\n",
    "generate_alert(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
